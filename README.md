# AssessorAI Crawler

Sistema de crawler para extraÃ§Ã£o de proposiÃ§Ãµes legislativas de assembleias estaduais brasileiras, desenvolvido com Scrapy e integraÃ§Ã£o com Weaviate.

## ğŸ—ï¸ Arquitetura do Projeto

```
assessorai_crawler/
â”œâ”€â”€ assessorai_crawler/          # CÃ³digo principal do Scrapy
â”‚   â”œâ”€â”€ spiders/                 # Spiders para cada estado
â”‚   â”‚   â”œâ”€â”€ proposicoeslegislapi.py  # Spider base (classe pai)
â”‚   â”‚   â”œâ”€â”€ proposicoessp.py         # SÃ£o Paulo
â”‚   â”‚   â”œâ”€â”€ proposicoesmg.py         # Minas Gerais
â”‚   â”‚   â””â”€â”€ ...                      # Outros estados
â”‚   â”œâ”€â”€ items.py                 # DefiniÃ§Ã£o dos dados estruturados
â”‚   â”œâ”€â”€ pipelines.py             # Processamento e validaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ settings.py              # ConfiguraÃ§Ãµes do Scrapy
â”‚   â””â”€â”€ utils.py                 # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ output/                      # JSONs gerados pelos crawlers
â”œâ”€â”€ importer.py                  # Script para importar dados no Weaviate
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â””â”€â”€ .env                        # VariÃ¡veis de ambiente
```

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### 1. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd assessorai_crawler

# Crie e ative um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

### 2. ConfiguraÃ§Ã£o das VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# Weaviate Configuration
WEAVIATE_URL="your-weaviate-cluster-url"
WEAVIATE_APIKEY="your-weaviate-api-key"
WEAVIATE_CLASS="Bill"

# OpenAI Configuration (para embeddings)
OPENAI_APIKEY="your-openai-api-key"
```

## ğŸ“Š Como Funciona

### 1. Estrutura de Dados

O projeto usa o item `ProposicaoItem` definido em `items.py`:

```python
ProposicaoItem:
- title: str          # TÃ­tulo da proposiÃ§Ã£o
- house: str          # Casa legislativa
- type: str           # Tipo (PL, PEC, etc.)
- number: int         # NÃºmero da proposiÃ§Ã£o
- year: int           # Ano
- author: list        # Lista de autores
- subject: str        # Ementa/assunto
- full_text: str      # Texto completo
- url: str            # URL pÃºblica
- uuid: str           # Identificador Ãºnico
- scraped_at: str     # Timestamp da coleta
```

### 2. Pipeline de Processamento

1. **ValidationPipeline**: Valida campos obrigatÃ³rios
2. **JsonWriterSinglePipeline**: Salva todos os itens em um Ãºnico JSON

## ğŸ•·ï¸ Como Desenvolver um Novo Crawler Web

### Metodologia: Do Site Ã  Estrutura de Dados

1. **ğŸ” Encontre a pÃ¡gina da casa legislativa**
   - Identifique o site oficial (ex: `www.al[uf].gov.br`)
   - Localize a seÃ§Ã£o de "ProposiÃ§Ãµes", "Projetos de Lei" ou similar

2. **ğŸ“‹ Encontre a pÃ¡gina que lista os projetos**
   - Busque por pÃ¡ginas de listagem (ex: `/proposicoes`, `/projetos`)
   - Analise a paginaÃ§Ã£o e filtros disponÃ­veis

3. **ğŸ”— Itere pela pÃ¡gina, buscando links para projetos individuais**
   - Identifique os seletores CSS/XPath dos links
   - Colete metadados bÃ¡sicos da listagem (tÃ­tulo, autor, data)

4. **ğŸ’¾ Armazene as variÃ¡veis necessÃ¡rias**
   - TÃ­tulo da proposiÃ§Ã£o
   - Tipo e nÃºmero (PL, PEC, etc.)
   - Autor(es)
   - Data de apresentaÃ§Ã£o
   - Ementa/assunto
   - URL pÃºblica

5. **ğŸ“„ FaÃ§a download da Ã­ntegra e converta para markdown**
   - Acesse pÃ¡gina individual do projeto
   - Extraia o texto completo (PDF, HTML, DOC)
   - Converta para markdown limpo

### Passo 1: Estrutura BÃ¡sica do Spider

```python
# assessorai_crawler/spiders/proposicoes[uf].py
import scrapy
import hashlib
from datetime import datetime
from urllib.parse import urljoin
from ..items import ProposicaoItem

class Proposicoes[UF]Spider(scrapy.Spider):
    name = 'proposicoes[uf]'
    house = 'Nome da Casa Legislativa'
    uf = '[uf]'
    slug = f'proposicoes{uf}'
    allowed_domains = ['www.al[uf].gov.br']
    start_urls = ['https://www.al[uf].gov.br/proposicoes']
    
    def parse(self, response):
        """Parse da pÃ¡gina de listagem de proposiÃ§Ãµes"""
        # Extrair links para proposiÃ§Ãµes individuais
        proposicao_links = response.css('selector-para-links::attr(href)').getall()
        
        for link in proposicao_links:
            full_url = urljoin(response.url, link)
            yield response.follow(full_url, self.parse_proposicao)
        
        # PaginaÃ§Ã£o
        next_page = response.css('selector-proxima-pagina::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
    
    def parse_proposicao(self, response):
        """Parse da pÃ¡gina individual da proposiÃ§Ã£o"""
        item = ProposicaoItem()
        
        # Extrair dados bÃ¡sicos
        item['title'] = response.css('h1.titulo::text').get('').strip()
        item['house'] = self.house
        item['url'] = response.url
        
        # Extrair tipo e nÃºmero do tÃ­tulo
        title_parts = item['title'].split()
        item['type'] = title_parts[0] if title_parts else ''
        
        # Extrair nÃºmero e ano (formato: "123/2024")
        if len(title_parts) > 1:
            try:
                num_year = title_parts[1].split('/')
                item['number'] = int(num_year[0])
                item['year'] = int(num_year[1])
            except (ValueError, IndexError):
                item['number'] = None
                item['year'] = None
        
        # Extrair outros campos
        item['author'] = self.extract_authors(response)
        item['subject'] = response.css('.ementa::text').get('').strip()
        item['presentation_date'] = self.extract_date(response)
        
        # Extrair texto completo
        texto_completo = self.extract_full_text(response)
        item['full_text'] = self.convert_to_markdown(texto_completo)
        item['length'] = len(item['full_text'] or '')
        
        # Metadados
        item['uuid'] = hashlib.md5(item['title'].encode('utf-8')).hexdigest()
        item['scraped_at'] = datetime.now().isoformat()
        
        yield item
    
    def extract_authors(self, response):
        """Extrai lista de autores"""
        authors_text = response.css('.autores::text').get('')
        return [a.strip() for a in authors_text.split(',') if a.strip()]
    
    def extract_date(self, response):
        """Extrai data de apresentaÃ§Ã£o"""
        date_text = response.css('.data-apresentacao::text').get('')
        # Implementar parsing de data especÃ­fico do site
        return date_text.strip()
    
    def extract_full_text(self, response):
        """Extrai texto completo da proposiÃ§Ã£o"""
        # MÃ©todo 1: Texto direto na pÃ¡gina
        full_text = response.css('.texto-completo').get()
        if full_text:
            return full_text
        
        # MÃ©todo 2: Link para PDF/DOC
        pdf_link = response.css('a[href*=".pdf"]::attr(href)').get()
        if pdf_link:
            # Fazer request para PDF e processar (ver seÃ§Ã£o de bibliotecas)
            pass
        
        return ''
    
    def convert_to_markdown(self, html_content):
        """Converte HTML para markdown limpo"""
        if not html_content:
            return ''
        
        # Usar biblioteca de conversÃ£o (ver seÃ§Ã£o de bibliotecas)
        # Exemplo com html2text
        import html2text
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = True
        return h.handle(html_content)
```

### Passo 2: Bibliotecas Ãšteis

Adicione ao `requirements.txt`:

```txt
# Parsing e extraÃ§Ã£o
beautifulsoup4          # Parsing HTML avanÃ§ado
lxml                    # Parser XML/HTML rÃ¡pido
selectolax              # Parser HTML ultrarrÃ¡pido

# ConversÃ£o de documentos
html2text               # HTML para Markdown
markdownify             # HTML para Markdown (alternativa)
pypandoc                # ConversÃ£o universal de documentos

# Processamento de PDF
PyPDF2                  # ExtraÃ§Ã£o de texto de PDF
pdfplumber              # PDF parsing avanÃ§ado
pymupdf                 # PDF processing (fitz)

# Processamento de texto
bleach                  # Limpeza de HTML
textract                # ExtraÃ§Ã£o de texto de vÃ¡rios formatos

# Utilidades web
requests-html           # Requests com suporte a JavaScript
selenium                # AutomaÃ§Ã£o de browser (para SPAs)
playwright              # Alternativa moderna ao Selenium
```

### Passo 3: PseudocÃ³digo Detalhado

```python
def develop_new_crawler():
    """
    Fluxo completo para desenvolver crawler de nova casa legislativa
    """
    
    # FASE 1: RECONHECIMENTO
    target_site = identify_legislative_house_website()
    propositions_section = find_propositions_listing_page(target_site)
    
    # FASE 2: ANÃLISE DA ESTRUTURA
    pagination_pattern = analyze_pagination(propositions_section)
    list_item_selectors = identify_list_item_selectors(propositions_section)
    individual_page_pattern = analyze_individual_pages(propositions_section)
    
    # FASE 3: EXTRAÃ‡ÃƒO DE METADADOS
    for page in paginate_through_listings(propositions_section):
        for item_link in extract_proposition_links(page):
            metadata = extract_basic_info_from_listing(item_link)
            
            # FASE 4: EXTRAÃ‡ÃƒO DE CONTEÃšDO COMPLETO
            individual_page = fetch_individual_page(item_link)
            full_content = extract_full_content(individual_page)
            
            # FASE 5: PROCESSAMENTO E LIMPEZA
            cleaned_content = clean_and_normalize_text(full_content)
            markdown_content = convert_to_markdown(cleaned_content)
            
            # FASE 6: ESTRUTURAÃ‡ÃƒO DE DADOS
            proposition_item = create_proposition_item(
                title=metadata['title'],
                house=target_site['house_name'],
                authors=metadata['authors'],
                date=metadata['date'],
                full_text=markdown_content,
                url=item_link
            )
            
            yield proposition_item

def extract_full_content(page_response):
    """EstratÃ©gias para extrair texto completo"""
    
    # ESTRATÃ‰GIA 1: Texto direto na pÃ¡gina HTML
    if has_direct_text_content(page_response):
        return extract_html_text(page_response)
    
    # ESTRATÃ‰GIA 2: Download de PDF
    elif has_pdf_link(page_response):
        pdf_url = get_pdf_link(page_response)
        pdf_content = download_and_extract_pdf(pdf_url)
        return pdf_content
    
    # ESTRATÃ‰GIA 3: Documento Word/DOC
    elif has_doc_link(page_response):
        doc_url = get_doc_link(page_response)
        doc_content = download_and_extract_doc(doc_url)
        return doc_content
    
    # ESTRATÃ‰GIA 4: ConteÃºdo carregado via JavaScript
    elif requires_javascript(page_response):
        js_content = extract_with_selenium(page_response.url)
        return js_content
    
    return ""
```

### Passo 4: ImplementaÃ§Ãµes EspecÃ­ficas por Tipo de ConteÃºdo

```python
# Para sites com PDF
def extract_pdf_content(pdf_url):
    """Extrai texto de PDF usando pdfplumber"""
    import pdfplumber
    import requests
    
    response = requests.get(pdf_url)
    with pdfplumber.open(BytesIO(response.content)) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Para sites com JavaScript/SPA
def extract_with_selenium(url):
    """Extrai conteÃºdo de sites com JavaScript"""
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    
    driver.get(url)
    # Aguardar carregamento
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "content"))
    )
    
    content = driver.find_element(By.CLASS_NAME, "texto-completo").text
    driver.quit()
    return content

# Para limpeza e conversÃ£o
def clean_and_convert_to_markdown(html_content):
    """Limpa HTML e converte para markdown"""
    import bleach
    import html2text
    
    # Limpar HTML malicioso/desnecessÃ¡rio
    clean_html = bleach.clean(
        html_content,
        tags=['p', 'br', 'strong', 'em', 'ul', 'ol', 'li', 'h1', 'h2', 'h3'],
        strip=True
    )
    
    # Converter para markdown
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = True
    h.body_width = 0  # Sem quebra de linha
    
    markdown = h.handle(clean_html)
    
    # Limpeza adicional
    markdown = re.sub(r'\n\n+', '\n\n', markdown)  # MÃºltiplas quebras
    markdown = markdown.strip()
    
    return markdown
```

### Passo 5: Ferramentas de Desenvolvimento e Debug

```python
# Ferramenta para anÃ¡lise de seletores CSS
def analyze_page_structure(url):
    """Analisa estrutura da pÃ¡gina para identificar seletores"""
    import requests
    from bs4 import BeautifulSoup
    
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Identificar possÃ­veis seletores para proposiÃ§Ãµes
    potential_selectors = [
        'a[href*="proposicao"]',
        'a[href*="projeto"]', 
        'a[href*="pl"]',
        '.proposicao-item a',
        '.projeto-link',
        'tr td a'  # Para tabelas
    ]
    
    for selector in potential_selectors:
        elements = soup.select(selector)
        if elements:
            print(f"Selector '{selector}' encontrou {len(elements)} elementos")
            for i, elem in enumerate(elements[:3]):  # Primeiros 3
                print(f"  {i+1}: {elem.get('href')} - {elem.text.strip()}")

# Ferramenta para testar extraÃ§Ã£o
def test_extraction(url, selectors_dict):
    """Testa seletores em uma pÃ¡gina especÃ­fica"""
    import requests
    from bs4 import BeautifulSoup
    
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    results = {}
    for field, selector in selectors_dict.items():
        try:
            element = soup.select_one(selector)
            results[field] = element.text.strip() if element else 'NOT FOUND'
        except Exception as e:
            results[field] = f'ERROR: {str(e)}'
    
    return results

# Exemplo de uso das ferramentas
if __name__ == "__main__":
    # Analisar estrutura da pÃ¡gina de listagem
    analyze_page_structure("https://www.alxx.gov.br/proposicoes")
    
    # Testar extraÃ§Ã£o em pÃ¡gina individual
    test_selectors = {
        'title': 'h1.titulo',
        'authors': '.autores',
        'date': '.data-apresentacao',
        'subject': '.ementa',
        'full_text': '.texto-completo'
    }
    
    results = test_extraction(
        "https://www.alxx.gov.br/proposicao/123", 
        test_selectors
    )
    
    for field, value in results.items():
        print(f"{field}: {value}")
```

### Passo 6: EstratÃ©gias por Tipo de Site

```python
# TIPO 1: Sites estÃ¡ticos simples (HTML tradicional)
class SimpleHTMLSpider(scrapy.Spider):
    """Para sites com HTML estÃ¡tico e estrutura simples"""
    
    def parse_static_listing(self, response):
        # Seletores diretos funcionam bem
        links = response.css('a.proposicao-link::attr(href)').getall()
        for link in links:
            yield response.follow(link, self.parse_proposicao)

# TIPO 2: Sites com paginaÃ§Ã£o AJAX
class AjaxPaginationSpider(scrapy.Spider):
    """Para sites que carregam mais conteÃºdo via AJAX"""
    
    def parse_ajax_pagination(self, response):
        # Interceptar requests AJAX
        import json
        
        # Primeira pÃ¡gina normal
        yield from self.parse_static_listing(response)
        
        # PÃ¡ginas AJAX subsequentes
        ajax_url = "https://site.gov.br/api/proposicoes"
        for page in range(2, 100):  # Ajustar limite
            yield scrapy.Request(
                f"{ajax_url}?page={page}",
                callback=self.parse_ajax_response,
                headers={'X-Requested-With': 'XMLHttpRequest'}
            )
    
    def parse_ajax_response(self, response):
        data = json.loads(response.text)
        for item in data.get('items', []):
            yield response.follow(item['url'], self.parse_proposicao)

# TIPO 3: Sites Single Page Application (SPA)
class SPASpider(scrapy.Spider):
    """Para sites React/Vue/Angular"""
    
    def __init__(self):
        # Requer Selenium ou Playwright
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        options = Options()
        options.add_argument('--headless')
        self.driver = webdriver.Chrome(options=options)
    
    def parse_spa_content(self, response):
        self.driver.get(response.url)
        
        # Aguardar carregamento
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "proposicao-item"))
        )
        
        # Extrair links apÃ³s JavaScript executar
        elements = self.driver.find_elements(By.CSS_SELECTOR, "a.proposicao-link")
        for element in elements:
            url = element.get_attribute('href')
            yield scrapy.Request(url, callback=self.parse_proposicao)
    
    def closed(self, reason):
        self.driver.quit()
```

## ğŸƒâ€â™‚ï¸ Executando os Crawlers

### Executar um Spider EspecÃ­fico

```bash
# Executar spider de SÃ£o Paulo
scrapy crawl proposicoessp

# Executar spider de Minas Gerais
scrapy crawl proposicoesmg

# Ver lista de todos os spiders
scrapy list
```

### Executar com ConfiguraÃ§Ãµes EspecÃ­ficas

```bash
# Salvar em formato especÃ­fico
scrapy crawl proposicoessp -o output/sp_dados.json

# Executar com log especÃ­fico
scrapy crawl proposicoessp -L INFO

# Executar em modo debug
scrapy crawl proposicoessp -L DEBUG

# Limitar nÃºmero de itens (para testes)
scrapy crawl proposicoessp -s CLOSESPIDER_ITEMCOUNT=10

# Configurar delay entre requests
scrapy crawl proposicoessp -s DOWNLOAD_DELAY=2
```

### Exemplos PrÃ¡ticos de Desenvolvimento

```bash
# 1. Criar novo spider interativamente
scrapy genspider proposicoesgo www.assembleia.go.gov.br

# 2. Testar seletores com scrapy shell
scrapy shell "https://www.assembleia.go.gov.br/proposicoes"

# 3. Debug especÃ­fico de uma pÃ¡gina
scrapy shell "https://www.assembleia.go.gov.br/proposicao/12345"

# 4. Executar com configuraÃ§Ãµes de desenvolvimento
scrapy crawl proposicoesgo \
  -s DOWNLOAD_DELAY=1 \
  -s CLOSESPIDER_ITEMCOUNT=5 \
  -L DEBUG
```

### Comandos Ãšteis no Scrapy Shell

```python
# No scrapy shell, use estes comandos para testar:

# Testar seletores CSS
response.css('a.proposicao-link').getall()
response.css('h1.titulo::text').get()

# Testar XPath
response.xpath('//a[contains(@href, "proposicao")]/@href').getall()

# Seguir link e testar
fetch('https://site.gov.br/proposicao/123')
response.css('.texto-completo::text').get()

# Testar regex
import re
title = "PL 123/2024"
match = re.match(r'(\w+)\s+(\d+)/(\d+)', title)
if match:
    print(f"Tipo: {match.group(1)}, NÃºmero: {match.group(2)}, Ano: {match.group(3)}")
```

## ğŸ“¤ Importando Dados para o Weaviate

ApÃ³s executar os crawlers, use o script de importaÃ§Ã£o:

```bash
# Importar dados de um estado especÃ­fico
python importer.py output/proposicoessp_proposicoes.json

# Importar com configuraÃ§Ãµes especÃ­ficas
python importer.py output/proposicoessp_proposicoes.json --max-tokens 4000 --overlap 200
```

### Funcionalidades do Importer

- **Chunking inteligente**: Divide textos longos em chunks baseados em tokens
- **DeduplicaÃ§Ã£o**: Evita importar dados duplicados usando UUIDs
- **Progress bar**: Mostra progresso da importaÃ§Ã£o
- **Controle de tokens**: Configura tamanho mÃ¡ximo de chunks para embeddings

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Modificar Pipelines

Em `settings.py`, vocÃª pode ajustar a ordem e configuraÃ§Ã£o dos pipelines:

```python
ITEM_PIPELINES = {
    "assessorai_crawler.pipelines.ValidationPipeline": 100,      # ValidaÃ§Ã£o
    "assessorai_crawler.pipelines.JsonWriterSinglePipeline": 300, # Escrita JSON
}
```

### Adicionar Novos Pipelines

Crie novos pipelines em `pipelines.py`:

```python
class CustomPipeline:
    def process_item(self, item, spider):
        # Sua lÃ³gica personalizada
        return item
```

### Debug e Logs

Configure logs em `settings.py`:

```python
# NÃ­vel de log
LOG_LEVEL = 'INFO'  # DEBUG, INFO, WARNING, ERROR

# Arquivo de log
LOG_FILE = 'scrapy.log'
```

## ğŸ§ª Testando Novos Spiders

### 1. Teste BÃ¡sico

```bash
# Teste seco (sem executar)
scrapy check proposicoes[uf]

# Teste com poucos itens
scrapy crawl proposicoes[uf] -s CLOSESPIDER_ITEMCOUNT=10
```

### 2. ValidaÃ§Ã£o de Dados

```bash
# Verificar se JSON foi gerado
ls -la output/

# Validar estrutura do JSON
python -m json.tool output/proposicoes[uf]_proposicoes.json
```

### 3. Debug de Items

Adicione logs no seu spider:

```python
def parse(self, response):
    for item in super().parse(response):
        self.logger.info(f"Item processado: {item['title']}")
        yield item
```

## ğŸ“‹ Checklist para Novo Estado

### Fase 1: AnÃ¡lise e Planejamento
- [ ] Identificar site oficial da casa legislativa
- [ ] Encontrar seÃ§Ã£o de proposiÃ§Ãµes/projetos de lei
- [ ] Analisar estrutura da pÃ¡gina de listagem
- [ ] Identificar sistema de paginaÃ§Ã£o
- [ ] Verificar se requer JavaScript (SPA)
- [ ] Testar seletores com `scrapy shell`

### Fase 2: Desenvolvimento
- [ ] Criar arquivo `proposicoes[uf].py` no diretÃ³rio `spiders/`
- [ ] Definir `name`, `house`, `uf` e configuraÃ§Ãµes bÃ¡sicas
- [ ] Implementar `parse()` para listagem
- [ ] Implementar `parse_proposicao()` para pÃ¡ginas individuais
- [ ] Implementar extraÃ§Ã£o de texto completo
- [ ] Implementar conversÃ£o para markdown

### Fase 3: Testes
- [ ] Testar spider com poucos items (`CLOSESPIDER_ITEMCOUNT=5`)
- [ ] Validar extraÃ§Ã£o de todos os campos obrigatÃ³rios
- [ ] Verificar qualidade da conversÃ£o para markdown
- [ ] Testar paginaÃ§Ã£o completa
- [ ] Verificar tratamento de erros

### Fase 4: ValidaÃ§Ã£o
- [ ] Executar coleta completa
- [ ] Validar JSON de saÃ­da
- [ ] Verificar URLs pÃºblicas funcionais
- [ ] Testar importaÃ§Ã£o no Weaviate
- [ ] Documentar peculiaridades do estado

### Fase 5: DocumentaÃ§Ã£o
- [ ] Documentar seletores especÃ­ficos usados
- [ ] Documentar estrutura particular do site
- [ ] Documentar problemas encontrados e soluÃ§Ãµes
- [ ] Atualizar README se necessÃ¡rio

## ğŸ› Problemas Comuns

### Problemas de Seletores CSS/XPath

**Seletores nÃ£o encontram elementos:**
```python
# âŒ Problema: Seletor muito especÃ­fico
response.css('div.container > div.content > table.proposicoes > tr > td > a')

# âœ… SoluÃ§Ã£o: Seletor mais genÃ©rico
response.css('a[href*="proposicao"]')
```

**Elementos carregados via JavaScript:**
```python
# âŒ Problema: ConteÃºdo nÃ£o existe no HTML inicial
response.css('.proposicao-dinamica')  # Retorna vazio

# âœ… SoluÃ§Ã£o: Usar Selenium
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(response.url)
# Aguardar carregamento e extrair
```

### Problemas de Encoding

**Caracteres especiais quebrados:**
```python
# âœ… SoluÃ§Ã£o: Configurar encoding correto
def parse(self, response):
    response = response.replace(encoding='utf-8')
    # ... resto do cÃ³digo
```

### Problemas de Rate Limiting

**Site bloqueia requests rÃ¡pidos:**
```python
# âœ… Configurar delay no settings.py
DOWNLOAD_DELAY = 2  # 2 segundos entre requests
RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Randomizar atÃ© 50%

# Ou no spider individual
custom_settings = {
    'DOWNLOAD_DELAY': 3,
    'CONCURRENT_REQUESTS': 1
}
```

### Problemas com PDFs

**PDF corrompido ou protegido:**
```python
def extract_pdf_safely(pdf_url):
    try:
        import pdfplumber
        response = requests.get(pdf_url)
        with pdfplumber.open(BytesIO(response.content)) as pdf:
            return "\n".join(page.extract_text() for page in pdf.pages)
    except Exception as e:
        # Fallback para OCR se necessÃ¡rio
        self.logger.warning(f"PDF extraction failed: {e}")
        return self.extract_pdf_with_ocr(pdf_url)
```

### Problemas de ValidaÃ§Ã£o

**Campos obrigatÃ³rios faltando:**
```python
# Verificar no pipeline se campos essenciais existem
def process_item(self, item, spider):
    required_fields = ['title', 'house', 'url', 'full_text']
    missing = [f for f in required_fields if not item.get(f)]
    
    if missing:
        spider.logger.warning(f"Missing fields: {missing}")
        # Decidir se descartar ou preencher com default
        for field in missing:
            item[field] = 'N/A'  # ou raise DropItem()
    
    return item
```

### Problemas de MemÃ³ria

**Spider consome muita memÃ³ria:**
```python
# âœ… Processar itens em lotes menores
custom_settings = {
    'CONCURRENT_REQUESTS': 1,
    'CLOSESPIDER_ITEMCOUNT': 1000,  # Parar apÃ³s 1000 itens
}

# Ou usar generator para texto muito grande
def extract_large_text(self, response):
    for chunk in self.process_text_in_chunks(response):
        yield chunk
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/novo-estado`)
3. Teste thoroughly o novo spider
4. Commit suas mudanÃ§as (`git commit -am 'Add spider for XX state'`)
5. Push para a branch (`git push origin feature/novo-estado`)
6. Abra um Pull Request

## ğŸ“ Notas Importantes

- **Dados sensÃ­veis**: Nunca commite arquivos `.env` ou chaves de API
- **Rate limiting**: Respeite os limites das APIs e sites (use `DOWNLOAD_DELAY`)
- **Robots.txt**: Sempre verifique e respeite o arquivo robots.txt do site
- **User-Agent**: Configure um User-Agent identificÃ¡vel e respeitoso
- **Testes**: Sempre teste com poucos items antes de executar coleta completa
- **URLs pÃºblicas**: Verifique se as URLs extraÃ­das sÃ£o acessÃ­veis publicamente  
- **Backup de dados**: FaÃ§a backup dos JSONs gerados antes de reprocessar
- **Monitoramento**: Sites podem mudar estrutura - monitore falhas regularmente
- **Legalidade**: Verifique se o crawling estÃ¡ em conformidade com os termos de uso
- **Performance**: Use `CONCURRENT_REQUESTS` e `DOWNLOAD_DELAY` apropriados

### Boas PrÃ¡ticas de Desenvolvimento

```python
# âœ… Sempre use try/catch para extraÃ§Ã£o
def extract_safely(self, response, selector, default=''):
    try:
        return response.css(selector).get('').strip()
    except Exception as e:
        self.logger.warning(f"Failed to extract {selector}: {e}")
        return default

# âœ… Valide dados antes de salvar
def validate_item(self, item):
    if not item.get('title'):
        return False
    if not item.get('full_text') or len(item['full_text']) < 100:
        return False
    return True

# âœ… Use logs informativos
def parse_proposicao(self, response):
    self.logger.info(f"Processing: {response.url}")
    item = self.extract_item(response)
    
    if self.validate_item(item):
        self.logger.info(f"Extracted: {item['title']}")
        yield item
    else:
        self.logger.warning(f"Invalid item from: {response.url}")
```

## ğŸ“š Recursos Ãšteis

- [DocumentaÃ§Ã£o do Scrapy](https://docs.scrapy.org/)
- [Weaviate Documentation](https://weaviate.io/developers/weaviate)
- [OpenAI API Documentation](https://platform.openai.com/docs)
